#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“å¯¼å‡ºå·¥å…·
å¯¼å‡ºå½“å‰æ•°æ®åº“çš„ç»“æ„å’Œå®Œæ•´æ•°æ®åˆ° JSON æ–‡ä»¶
"""

import sys
import io
import json
import os
from pathlib import Path
from datetime import datetime

# è§£å†³ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

from app import create_app, db
from sqlalchemy import inspect, text


def get_table_data(table_name: str) -> list:
    """è·å–è¡¨çš„æ‰€æœ‰æ•°æ®"""
    try:
        result = db.session.execute(text(f"SELECT * FROM {table_name}"))
        columns = result.keys()
        rows = []
        for row in result.fetchall():
            row_dict = {}
            for i, col in enumerate(columns):
                val = row[i]
                # å¤„ç†ç‰¹æ®Šç±»å‹
                if isinstance(val, bytes):
                    val = val.decode('utf-8', errors='replace')
                elif isinstance(val, datetime):
                    val = int(val.timestamp())
                row_dict[col] = val
            rows.append(row_dict)
        return rows
    except Exception as e:
        print(f"  âš ï¸  å¯¼å‡ºè¡¨ {table_name} å¤±è´¥: {e}")
        return []


def get_all_tables() -> list:
    """è·å–æ‰€æœ‰è¡¨å"""
    inspector = inspect(db.engine)
    return inspector.get_table_names()


def export_schema() -> dict:
    """å¯¼å‡ºæ•°æ®åº“ç»“æ„"""
    inspector = inspect(db.engine)
    schema = {}

    for table_name in inspector.get_table_names():
        columns = []
        for col in inspector.get_columns(table_name):
            col_info = {
                'name': col['name'],
                'type': str(col['type']),
                'nullable': col.get('nullable', True),
                'default': str(col.get('default')) if col.get('default') else None,
            }
            columns.append(col_info)

        # è·å–ä¸»é”®
        pk = inspector.get_pk_constraint(table_name)

        # è·å–å¤–é”®
        fks = inspector.get_foreign_keys(table_name)

        # è·å–ç´¢å¼•
        indexes = inspector.get_indexes(table_name)

        schema[table_name] = {
            'columns': columns,
            'primary_key': pk.get('constrained_columns', []) if pk else [],
            'foreign_keys': fks,
            'indexes': indexes,
        }

    return schema


def export_all_data(output_dir: Path):
    """å¯¼å‡ºæ‰€æœ‰è¡¨æ•°æ®åˆ°å•ç‹¬çš„ JSON æ–‡ä»¶"""
    tables = get_all_tables()

    # å®šä¹‰å¯¼å‡ºé¡ºåºï¼ˆè€ƒè™‘å¤–é”®ä¾èµ–ï¼‰
    export_order = [
        # åŸºç¡€é…ç½®è¡¨ï¼ˆæ— å¤–é”®ä¾èµ–ï¼‰
        'departments', 'permissions', 'roles', 'menus',
        'projects', 'sim_types', 'model_levels', 'fold_types',
        'solvers', 'solver_resources', 'status_defs', 'care_devices',
        'param_defs', 'output_defs', 'condition_defs',
        'automation_modules', 'workflows',
        # ç”¨æˆ·è¡¨
        'users',
        # å…³è”è¡¨
        'fold_type_sim_type_rels', 'working_conditions',
        'param_groups', 'param_group_param_rels',
        'condition_output_groups', 'cond_out_group_condition_rels', 'cond_out_group_output_rels',
        'param_tpl_sets', 'param_tpl_items', 'cond_out_sets',
        'project_sim_type_rels', 'sim_type_param_group_rels',
        'sim_type_cond_out_group_rels', 'sim_type_solver_rels',
        'user_project_permissions',
        # ä¸šåŠ¡æ•°æ®è¡¨
        'orders', 'order_results', 'sim_type_results', 'rounds',
    ]

    # æ·»åŠ æœªåœ¨åˆ—è¡¨ä¸­çš„è¡¨
    for table in tables:
        if table not in export_order:
            export_order.append(table)

    exported_data = {}

    for table in export_order:
        if table not in tables:
            continue
        print(f"  å¯¼å‡ºè¡¨: {table}...")
        data = get_table_data(table)
        if data:
            exported_data[table] = data
            print(f"    âœ“ {len(data)} æ¡è®°å½•")
        else:
            print(f"    - ç©ºè¡¨")

    return exported_data


def main():
    print("\n" + "=" * 60)
    print("ğŸš€ StructSim æ•°æ®åº“å¯¼å‡ºå·¥å…·")
    print("=" * 60)

    app = create_app()

    with app.app_context():
        output_dir = SCRIPT_DIR / 'exported-data'
        output_dir.mkdir(exist_ok=True)

        # æ˜¾ç¤ºå½“å‰æ•°æ®åº“ä¿¡æ¯
        current_db = str(db.engine.url)
        db_type = 'SQLite' if 'sqlite' in current_db else 'MySQL'
        print(f"\nğŸ“Š æ•°æ®åº“ç±»å‹: {db_type}")
        print(f"ğŸ“ è¿æ¥åœ°å€: {current_db}")

        # å¯¼å‡ºç»“æ„
        print("\nğŸ“‹ å¯¼å‡ºæ•°æ®åº“ç»“æ„...")
        schema = export_schema()
        schema_file = output_dir / 'schema.json'
        with open(schema_file, 'w', encoding='utf-8') as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ ç»“æ„å·²ä¿å­˜åˆ°: {schema_file}")

        # å¯¼å‡ºæ•°æ®
        print("\nğŸ“¦ å¯¼å‡ºæ•°æ®...")
        all_data = export_all_data(output_dir)

        # ä¿å­˜å®Œæ•´æ•°æ®åˆ°å•ä¸ªæ–‡ä»¶
        full_data_file = output_dir / 'full_data.json'
        with open(full_data_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"\n  âœ“ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: {full_data_file}")

        # ç»Ÿè®¡
        total_records = sum(len(data) for data in all_data.values())
        print(f"\nğŸ“Š å¯¼å‡ºç»Ÿè®¡:")
        print(f"  - è¡¨æ•°é‡: {len(all_data)}")
        print(f"  - æ€»è®°å½•æ•°: {total_records}")

        print("\n" + "=" * 60)
        print("âœ… å¯¼å‡ºå®Œæˆï¼")
        print("=" * 60 + "\n")


if __name__ == '__main__':
    main()
